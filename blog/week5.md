# Bias in Machine Learning

Since I have researched about the ethical issues of technology in college as a philosophy majored student, the concept of bias in reference of machine learning has been a main concern for years.

There are many recent observations show that AI is just as capable of bias as humans, even though they run on mathematics. The reason why AI tools are not free from bias because they learn from their human makers and their biases. At this point, it seems like just an extension of human nature and culture.

Quotes by "[Machine bias](https://www.technologyreview.com/s/608248/biased-algorithms-are-everywhere-and-no-one-seems-to-care/) is the growing body of research around the ways in which algorithms exhibit the bias of their creators or their input data. As machine learning is increasingly used across all industries, bias is being discovered with subtle and obvious consequences. It is important to note that machine learning algorithms rely on bias — statistical bias. These algorithms could not make any predictions at all if it wasn’t for the statistical bias required to make estimations about new data that the model has never seen before. Machine bias is different than statistical bias. Said simply — machine bias is programming that assumes the prejudice of its creators or data."

A several studies by universities and researchers found that machines showd biases similar to those found in the humans. For instance, the word “men” was associated with words like “work,” “math,” and “science” while the word “women” was associated with “family” and “the arts.” And young people, flowers, and musical instruments were considered more pleasant than old people, insects (Gil Grissom from “CSI” is the person who loves insects and enjoys racing cockroaches, which is not an appealing hobby) or weapons. As mentioned in the last class, this has been an issue for some time, as evidence of AI bias was found in recruiting, online advertising, and pricing. What’s surprising is that all of these are driven by so-called neutral algorithms. It is concerning because it could result in several problems. Not even Google is free from bias. When using Google Translate, the tool automatically converted gender-neutral pronouns into male or female. For instance, “doctor” was converted to “he” and nurse to “she.”
We can find others not only from language pretty easily.

The catastrophe that could ensue

We hardly need to imagine any of it. AI-powered tools have attracted bad press already because of ‘biased’ results. Some instances:

Google Photos auto-tagging African-Americans as apes in some images.
Microsoft Kinect having a tough time in recognizing people with dark skin tone.
Passport systems reading Asians’ images as people with their eyes closed and rejecting the uploaded images.
The impact of biases in AI is clearly hard-hitting. The problem is, it’s not only limited or skewed data that could create deep-rooted biases in machine learning algorithms driving AI applications. The biases could result from other sources.

Sources of biases in AI tools
Whereas some AI systems “learn” from the data you feed them, others learn from their interactions with real users. Instant messaging apps loaded with AI capabilities are the perfect example. In such cases, however, it’s very likely that the applications will develop the biases of the people whom it interacts with.

Microsoft’s Tay, a Twitter-based chatbot, was absolutely ruined as a result of its interactions with a user community that proved to be highly racist and misogynistic. The result — that chatbot started posting aggressive, racist, and misogynist tweets, causing Microsoft to promptly pull it down, after a mere 24 hours of going live. That chatbot could not think for itself, obviously — it needed to be given some commonsense and tact in its algorithm.

Emergent bias is another source, and though it’s one of the lesser known, it’s the scariest of them all. Consider Facebook: You like a certain kind of statuses, posts, and videos, and you can expect your timeline to be loaded with content “more like the same” as you liked earlier.

This is called “confirmation bias,” wherein the beliefs and values of an individual, related to anything (as trivial as one’s favorite TV series genre to as far-reaching as one’s political affiliations), get reinforced one day after another, purely because the individual is hardly exposed to contradictory information. When Elon Musk calls AI “an existential threat,” it stems a lot from the potential of AI to absolutely numb down the human ability to mature, evolve, change, and adjust, because of emergent biases.

Bias elimination
AI tools

To make sure these prejudices are not present in AI tools, a new category of jobs is about to be created: de-biasing AI tools. Once an AI tool has been created, it will be tested for biases by an expert who is trained in de-biasing machines. Areas like credit and lending have always suffered from human discrimination, and companies that deploy AI tools must be aware of the discriminations by these tools. Most companies are aware of the challenge, and have even started testing the machines to free them from biases.

What is most concerning is that consumers might never be able to fathom how machines can also turn discriminatory, and keep being treated unfairly without realizing it (scary really — conjures up images of Skynet!). To solve the problem, the people creating the tools and entering data in them must be free from biases — or at least know that they have them and make sure they aren’t imparted into the AI tools.

But ultimately, all data-driven tools created by humans will always have some amount of bias, and we have to live with it. There is nothing intrinsically wrong with profiling someone though — human beings do it even though some say it is immoral. Without the ability to profile, human evolution would have been curtailed several millennia ago. Detecting danger is part of what makes us human but being biased based on unethical reasons is concerning.

Not an ideal situation, not an ideal world
In the ideal world, machine-learning algorithms would be totally objective. They, however, learn from digital artifacts created by humans, with all their proclivities and biases. However, by acknowledging the existence of these biases, identifying their sources, and eliminating them, AI can be made a lot more objective.






Overall, the most influential components of a machine learning model are the algorithm and data used. A model is only as good as the data it learns from, and as we will see below, this is imperative to maintaining the integrity of decisions made from machine learning algorithms.

What is machine bias?


There have been a wide range of discoveries of biased machine learning outcomes over the last few years in correlation with the growing use of the technology. As much as machine learning offers significant increases in convenience, oftentimes this benefits some to the detriment of others.

Every company that builds technology in any capacity is at risk of incorporating the prejudice of its creators.


What is the impact of machine bias?
Machine bias is increasingly impactful due to its expansive uses in the modern world. Algorithms demonstrating machine bias may harm human life in an unfair capacity. Often this happens when the list of data categories is too limited, or inappropriate or invalid personal data is used. Some of these consequences may seem hypothetical, innocuous, and long term, while others are immediate and direct.

The immediate consequences of machine learning can be observed from the growing use of these algorithms in law enforcement. From research conducted by ProPublica, a non-profit research institution, it was found that COMPAS, a machine learning algorithm used to determine criminal defendants’ likelihood to recommit crimes, was biased in how it made predictions. The algorithm is used by judges in over a dozen states to make decisions on pre-trial conditions, and sometimes, in actual sentencing. ProPublica found that the algorithm was two times more likely to incorrectly predict black defendants were high risk for recommitting a crime, and conversely two times more likely to incorrectly predict white defendants were low risk for recommitting a crime. This difference could (and did) mean defendants were held before trial or let out on bail, or were convicted with a tougher or more lenient sentence.

Facial recognition is a well-known form of machine learning. Using facial recognition software, common on social media platforms like Facebook or with Google Photo, an algorithm can determine a face it has seen before in a photo by name. In July 2015, Jacky Alcine posted to Twitter a Google Photo that used facial recognition tagging him and another friend as gorillas [source]. The machine learning algorithm in the Google Photo software learned incorrectly. Most likely, the data used to train the algorithm did not include substantial training for facial appearances of all kinds.

Similarly, when you surf the web or interact with various social media platforms, it is common to see advertisements along the side, top, or bottom of your web page. More often than not, in recent years, these ads are tailored to your specific interests, needs, and wants — as determined by your web search history. Through the use of marketing automation tools, it is possible to track a user’s interaction with the website they are on, and anything they do from a given website. This gathered data is captured and analyzed with machine learning algorithms, which then predict what types of ads you’d like to see based on your interactions online. A study by AdFisher recently revealed that men were six times more likely than women to see Google ads for high paying jobs [source]. The immediate consequence of this machine bias is that a woman may not see a high paying job and therefore is less likely to know about it and apply. The long-term result could mean more ingrained gender discrepancies in high ranking positions.


Using ethics to solve machine bias
The first step to correcting bias that results from machine learning algorithms is acknowledging the bias exists. Researchers have been discussing ethical machine making since as early as 1985, when James Moor defined implicit and explicit ethical agents. Implicit agents are ethical because of their inherent programming or purpose. Explicit agents are machines given principles or examples to learn from to make ethical decisions in uncertain or unknown circumstances.

It is imperative that the AI community emphasize the use of machine ethics to prevent and correct for bias in machine learning algorithms. There are three primary ways that ethics can be used to mitigate negative unfairness in algorithmic programming: technical, political, and social.

As machine learning and AI experts say, “garbage in, garbage out” [source]. Proactive or retroactive efforts can be taken to find technical solutions within the code used to conduct machine learning. To save time, energy, and resources, it is preferable to take proactive measures to avoid bias in the first place. This includes measures such as finding comprehensive data, experimenting with different datasets and metrics, increased representation in the technical workforce, external validity testing, and auditing. When protected attributes like age, gender, and race are factors in an algorithm, it is important to incorporate them while also addressing the social bias that may come from particular attributes within the code. Obviously, this is a sensitive and complicated task but is not impossible. Exciting research was done last year to test this very goal [source].

Political pressure can be used to incentivize resource allocation to ethical machine learning creation. The EU passed the General Data Protection Regulation (GDPR) to take effect in 2018, which outlines citizens’ “right to explanation” regarding machine learning decisions made about them. The Obama administration also formally pushed for investigation of big data and machine learning algorithms to ensure fairness in 2016. Policies and publicity from political figures give more weight to the value and importance of ethical machine learning creation.

Lastly, social awareness can make an enormous difference in correcting for the far-reaching impacts of bias in machine learning. To date, much of the prejudice found in machine learning algorithms has been identified by the very users it hurts. Educating users about machine learning, how it is used and why, empowers consumers to demand higher ethical standards and transparent practices from the corporations that serve them. Community groups such as the Algorithmic Justice League (AJL) founded by Joy Buolamwini, help to promote crowd-sourced reporting and the study of bias in machine learning and other technologies. Involvement from diverse populations in the ethical creation and consumption of machine learning predictions will lead to further progress in ethics that include all users.

It is up to all of us to determine the path machine learning algorithms take and how well they ultimately serve our highest purposes. If we carefully consider the prejudices we inherently carry when creating these technologies and correct for them, creators and the companies they lead will harness the real power of machine learning to the benefit of corporations and consumers.


Thank you for reading! If you have any questions or ideas on what you’d like to hear more about regarding ethics in machine learning and AI, please feel free to comment below or shoot me a message: nicole.shadowen@gmail.com.


